{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of b-quark jets in the Aleph simulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python macro for selecting b-jets (sprays of particles with a b-quark in it) in Aleph Z -> qqbar MC (i.e. simulated decays of the Z0 boson decaying to a quark and an anti-quark) in various ways:\n",
    "* Initially, simply with \"if\"-statements making requirements on certain variables. This corresponds to selecting \"boxes\" in the input variable space (typically called \"X\"). One could also try a Fisher discriminant (linear combination of input variables), which corresponds to a plane in the X-space. But as the problem is non-linear, it is likely to be sub-optimal.\n",
    "\n",
    "* Next using Machine Learning (ML) methods. We will during the first week try both (Boosted) Decision Tree ((B)DT) based and Neural Net (NN) based methods, and see how complicated (or not) it is to get a good solution, and how much better it performs compared to the \"classic\" selection method.\n",
    "\n",
    "Once you obtain a classification of b-jets vs. non-b-jets, think about how to quantify the quality of your algorithm. Also, try to compare it to the NN result of the Aleph collaboration, given by the variable \"nnbjet\". It is based on a neural network with 6 input variables (prob_b, spheri, pt2rel, multip, bqvjet, and ptlrel), and two hidden layers each with 10 nodes in. Can you do better?\n",
    "\n",
    "In the end, this exercise is the simple start on moving into the territory of Machine Learning analysis.\n",
    "\n",
    "\n",
    "### Data:\n",
    "The input variables (X) are (where Aleph uses only the first six):\n",
    "* **prob_b**: Probability of being a b-jet from the pointing of the tracks to the vertex.\n",
    "* **spheri**: Sphericity of the event, i.e. how spherical it is.\n",
    "* **pt2rel**: The transverse momentum squared of the tracks relative to the jet axis, i.e. width of the jet.\n",
    "* **multip**: Multiplicity of the jet (in a relative measure).\n",
    "* **bqvjet**: b-quark vertex of the jet, i.e. the probability of a detached vertex.\n",
    "* **ptlrel**: Transverse momentum (in GeV) of possible lepton with respect to jet axis (about 0 if no leptons).\n",
    "* energy: Measured energy of the jet in GeV. Should be 45 GeV, but fluctuates.\n",
    "* cTheta: cos(theta), i.e. the polar angle of the jet with respect to the beam axis. Note, that the detector works best in the central region (|cTheta| small) and less well in the forward regions.\n",
    "* phi:    The azimuth angle of the jet. As the detector is uniform in phi, this should not matter (much).\n",
    "\n",
    "The target variable (Y) is:\n",
    "* isb:    1 if it is from a b-quark and 0, if it is not.\n",
    "\n",
    "Finally, those before you (the Aleph collaboration in the mid 90'ies) produced a Neural Net (6 input variables, two hidden layers with 10 neurons in each, and 1 output varible) based classification variable, which you can compare to (and compete with?):\n",
    "* nnbjet: Value of original Aleph b-jet tagging algorithm, using only the last six variables (for reference).\n",
    "\n",
    "\n",
    "### Task:\n",
    "Thus, the task before you is to produce functions (non-ML and then ML algorithm), which given the input variables X provides an output variable estimate, Y_est, which is \"closest possible\" to the target variable, Y. The \"closest possible\" is left to the user to define in a _Loss Function_, which we will discuss further. In classification problems (such as this), the typical loss function to use \"Cross Entropy\", see https://en.wikipedia.org/wiki/Cross_entropy.\n",
    "\n",
    "Once you have results, you're welcome to continue with a Fisher Linear Discriminant, and you may also challenge yourself by considering \"v1\" of the data, which is a little less \"polished and ready\". If you also manage this, then don't hold back in applying a real ML algorithm to the problem (you can get inspiration from \"ML_MethodsDemos.ipynb\" or the vast internet). A suggestion might be XGBoost or LightGBM.\n",
    "\n",
    "* Author: Troels C. Petersen (NBI)\n",
    "* Email:  petersen@nbi.dk\n",
    "* Date:   17th of April 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division   # Ensures Python3 printing & division standard\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.colors import LogNorm\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Possible other packages to consider: cornerplot, seaplot, sklearn.decomposition(PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random\n",
    "r.seed(42)\n",
    "\n",
    "SavePlots = False\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate an attempt at classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is made into a function, as this is called many times. It returns a \"confusion matrix\" and the fraction of wrong classifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(bquark) :\n",
    "    N = [[0,0], [0,0]]   # Make a list of lists (i.e. confusion matrix) for counting successes/failures.\n",
    "    for i in np.arange(len(isb)):\n",
    "        if (bquark[i] == 0 and isb[i] == 0) : N[0][0] += 1\n",
    "        if (bquark[i] == 0 and isb[i] == 1) : N[0][1] += 1\n",
    "        if (bquark[i] == 1 and isb[i] == 0) : N[1][0] += 1\n",
    "        if (bquark[i] == 1 and isb[i] == 1) : N[1][1] += 1\n",
    "    fracWrong = float(N[0][1]+N[1][0])/float(len(isb))\n",
    "    accuracy = 1.0 - fracWrong\n",
    "    return N, accuracy, fracWrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Main program start:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data, and make arrays of each variable (and understand these!).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data (with this very useful NumPy reader):\n",
    "data = np.genfromtxt('AlephBtag_MC_train_Nev5000.csv', names=True)    # For faster running\n",
    "# data = np.genfromtxt('AlephBtag_MC_train_Nev50000.csv', names=True)   # For more data\n",
    "\n",
    "# Kinematics (energy and direction) of the jet:\n",
    "energy = data['energy']\n",
    "cTheta = data['cTheta']\n",
    "phi    = data['phi']\n",
    "\n",
    "# Classification variables (those used in Aleph's NN):\n",
    "prob_b = data['prob_b']\n",
    "spheri = data['spheri']\n",
    "pt2rel = data['pt2rel']\n",
    "multip = data['multip']\n",
    "bqvjet = data['bqvjet']\n",
    "ptlrel = data['ptlrel']\n",
    "\n",
    "# Aleph's NN score:\n",
    "nnbjet = data['nnbjet']\n",
    "\n",
    "# Truth variable whether it really was a b-jet or not (i.e. target)\n",
    "isb    = data['isb']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the data:\n",
    "Define the histogram range and binning (important - programs are generally NOT good at this):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nbins = 100\n",
    "xmin = 0.0\n",
    "xmax = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make new lists selected based on what the jets really are (b-quark jet or light-quark jet):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_b_bjets = prob_b[isb == 1]\n",
    "prob_b_ljets = prob_b[isb == 0]\n",
    "bqvjet_bjets = bqvjet[isb == 1]\n",
    "bqvjet_ljets = bqvjet[isb == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce 1D figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce the actual figure, here with two histograms in it:\n",
    "fig, ax = plt.subplots(figsize=(12, 6))      # Create just a single figure and axes (figsize is in inches!)\n",
    "hist_prob_b_bjets = ax.hist(prob_b_bjets, bins=Nbins, range=(xmin, xmax), histtype='step', linewidth=2, label='prob_b_bjets', color='blue')\n",
    "hist_prob_b_ljets = ax.hist(prob_b_ljets, bins=Nbins, range=(xmin, xmax), histtype='step', linewidth=2, label='prob_b_ljets', color='red')\n",
    "ax.set_xlabel(\"Probability of b-quark based on track impact parameters\")     # Label of x-axis\n",
    "ax.set_ylabel(\"Frequency / 0.01\")                                            # Label of y-axis\n",
    "ax.set_title(\"Distribution of prob_b\")                                       # Title of plot\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend(loc='best')                                                        # Legend. Could also be 'upper right'\n",
    "ax.grid(axis='y')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "\n",
    "if SavePlots :\n",
    "    fig.savefig('Hist_prob_b_and_bqvjet.pdf', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce 2D figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we try a scatter plot, to see how the individual events distribute themselves:\n",
    "fig2, ax2 = plt.subplots(figsize=(12, 6))\n",
    "scat2_prob_b_vs_bqvjet_bjets = ax2.scatter(prob_b_bjets, bqvjet_bjets, label='b-jets', color='blue')\n",
    "scat2_prob_b_vs_bqvjet_ljets = ax2.scatter(prob_b_ljets, bqvjet_ljets, label='l-jets', color='red')\n",
    "ax2.legend(loc='best')\n",
    "fig2.tight_layout()\n",
    "fig2.show()\n",
    "\n",
    "if SavePlots :\n",
    "    fig2.savefig('Scatter_prob_b_vs_bqvjet.pdf', dpi=600)\n",
    "\n",
    "\n",
    "# However, as can be seen in the figure, the overlap between b-jets and light-jets is large,\n",
    "# and one covers much of the other in a scatter plot, which also does not show the amount of\n",
    "# statistics in the dense regions. Therefore, we try two separate 2D histograms (zoomed):\n",
    "fig3, ax3 = plt.subplots(1, 2, figsize=(12, 6))\n",
    "hist2_prob_b_vs_bqvjet_bjets = ax3[0].hist2d(prob_b_bjets, bqvjet_bjets, bins=[40,40], range=[[0.0, 0.4], [0.0, 0.4]])\n",
    "hist2_prob_b_vs_bqvjet_ljets = ax3[1].hist2d(prob_b_ljets, bqvjet_ljets, bins=[40,40], range=[[0.0, 0.4], [0.0, 0.4]])\n",
    "ax3[0].set_title(\"b-jets\")\n",
    "ax3[1].set_title(\"light-jets\")\n",
    "\n",
    "fig3.tight_layout()\n",
    "fig3.show()\n",
    "\n",
    "if SavePlots :\n",
    "    fig3.savefig('Hist2D_prob_b_vs_bqvjet.pdf', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I give the selection cuts names, so that they only need to be changed in ONE place (also ensures consistency!):\n",
    "# Note: This is where you change things (selection values and adding variables) to improve the performance:\n",
    "cut_bqvjet = 0.20\n",
    "\n",
    "# Think about how the above cuts \"works\", and then imagine what I have in mind below, trying to refine the selection:\n",
    "# cut_propb = 0.15\n",
    "# loose_propb = 0.07\n",
    "# tight_propb = 0.26\n",
    "# loose_bqvjet = 0.09\n",
    "# tight_bqvjet = 0.34\n",
    "\n",
    "# If prob_b indicate b-quark, call it a b-quark, otherwise not!\n",
    "bquark=[]\n",
    "for i in np.arange(len(prob_b)):\n",
    "    if (bqvjet[i] > cut_bqvjet) :\n",
    "        bquark.append(1)\n",
    "    else :\n",
    "        bquark.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, accuracy, fracWrong = evaluate(bquark)\n",
    "print(\"\\nRESULT OF HUMAN ATTEMPT AT A GOOD SELECTION:\")\n",
    "print(\"  First number in parenthesis is the estimate, second is the MC truth:\")\n",
    "print(\"  True-Negative (0,0)  = \", N[0][0])\n",
    "print(\"  False-Negative (0,1) = \", N[0][1])\n",
    "print(\"  False-Positive (1,0) = \", N[1][0])\n",
    "print(\"  True-Positive (1,1)  = \", N[1][1])\n",
    "print(\"    Fraction wrong            = ( (0,1) + (1,0) ) / sum = \", fracWrong)\n",
    "print(\"    Fraction right (accuracy) = ( (0,0) + (1,1) ) / sum = \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with Aleph NN-approach from 1990'ies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bquark=[]\n",
    "for i in np.arange(len(prob_b)):\n",
    "    if   (nnbjet[i] > 0.82) : bquark.append(1)\n",
    "    else : bquark.append(0)\n",
    "\n",
    "N, accuracy, fracWrong = evaluate(bquark)\n",
    "print(\"\\nALEPH BJET TAG:\")\n",
    "print(\"  First number in parenthesis is the estimate, second is the MC truth:\")\n",
    "print(\"  True-Negative (0,0)  = \", N[0][0])\n",
    "print(\"  False-Negative (0,1) = \", N[0][1])\n",
    "print(\"  False-Positive (1,0) = \", N[1][0])\n",
    "print(\"  True-Positive (1,1)  = \", N[1][1])\n",
    "print(\"    Fraction wrong            = ( (0,1) + (1,0) ) / sum = \", fracWrong)\n",
    "print(\"    Fraction right (accuracy) = ( (0,0) + (1,1) ) / sum = \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suggested problems:\n",
    "\n",
    "1. Start by plotting the six \"Aleph classification variables\" for signal and background, and see which seems to separate most. Possibly draw ROC curves for all of these separately, to quantify this.\n",
    "\n",
    "2. As you can see, the Aleph-NN performs significantly better. Try to optimize your cuts, combining several of them in smart ways. You may also want to make a linear combination (i.e. Fisher Linear Discriminant) to do better. Challenge yourself to push it as far as you can (well, for say 30 minutes).\n",
    "\n",
    "3. Does including more data (50000 instead of 5000 events) help your performance?\n",
    "\n",
    "4. Currently, the scoring (also called the loss function) is simply done by considering the fraction of wrong estimates. Think about what the alternatives could be, especially if you were to give a continuous score like the Aleph-NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning points:\n",
    "\n",
    "From this exercise you should get a feel for the problem at hand, namely how to separate two populations in a 6-dimensional space. It is hard to imagine, yet with simple cuts you should be able to get \"some performance\", though never anywhere close to the Aleph-NN. You should learn, that it is hard, but that at least the fact that you have known cases makes you capable of getting that \"some performance\". And you should of course be able to draw ROC curves to compare performances.\n",
    "\n",
    "The next steps (i.e. following exercises) are to improve this performance through the use of Machine Learning (ML), and make you capable not only of getting results, but also confident in optimising them, and certainly proficient in interpreting them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Solution(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division   # Ensures Python3 printing & division standard\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.colors import LogNorm\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data\n",
    "\n",
    "We read in the data in a slightly different way, better suited for ML-based approach \n",
    "...and we choose input and target variables:\n",
    "\n",
    "A few worlds on the variables and feature choices:\n",
    "* 'isb' is our binary truth variables. If isb = 1 then it's a b-quark and isb = 0 if it is not.\n",
    "   Because this is our truth, we must not include it as the input to our model.\n",
    "* 'nnbjet' is our \"competitor\" e.g. a model we are supposed to benchmark against.\n",
    "   Therefore 'nnbjet' shouldn't be in our input either.\n",
    "* 'energy', 'cTheta', and 'phi' are kinematic variables of the jet, and not about the jet type.\n",
    "   Though they might help, they were not in the original Aleph NN (nnbjet), and to compare, we omit them.\n",
    "\n",
    "Usually one would apply further checks/regularization/standardization of data at this step, but this data has already been \"prepared\", so we'll move onto seperate the data into input, truth and benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We import the data slightly more convenient here\n",
    "import pandas as pd \n",
    "from pandas import Series, DataFrame \n",
    "\n",
    "# Read the data in using Pandas Dataframe and print the variables:\n",
    "data = pd.DataFrame(np.genfromtxt('AlephBtag_MC_train_Nev5000.csv', names=True))\n",
    "#data = pd.DataFrame(np.genfromtxt('AlephBtag_MC_train_Nev50000.csv', names=True))\n",
    "\n",
    "variables = data.columns\n",
    "print(variables)\n",
    "\n",
    "# Decide on which variables to use for input (X) and what defines the label (Y):\n",
    "input_variables = variables[(variables != 'nnbjet') & (variables != 'isb') & (variables != 'energy') & (variables != 'cTheta') & (variables != 'phi')]\n",
    "input_data      = data[input_variables]\n",
    "truth_data      = data['isb']\n",
    "benchmark_data  = data['nnbjet']\n",
    "print(\"  Variables used for training: \", input_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Classify B-jets using LightGBM:\n",
    "\n",
    "This is a solution example using LightGBM (tree based)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "# Split data set into training and test set and feed these to LightGBM:\n",
    "# We choose a 75:25 division here. Dataset is shuffeled before the split. \n",
    "# By using a number (42), it creates a random seed so you can rerun and obtain the same result.\n",
    "input_train, input_test, truth_train, truth_test, benchmark_train, benchmark_test = train_test_split(input_data, truth_data, benchmark_data, test_size=0.25, random_state=42)\n",
    "lgb_train = lgb.Dataset(input_train, truth_train)\n",
    "lgb_eval  = lgb.Dataset(input_test,  truth_test, reference=lgb_train)\n",
    "params = {\n",
    "    'boosting_type': 'gbdt', #Traditional Gradient Boosting tree, we are combining many 'weak' learners here!\n",
    "    'objective': 'binary', #The outcome is binary, b-quark or not\n",
    "    'num_leaves': 6, #Set a maximum tree leaves to avoid overfitting\n",
    "    'verbose':-1, #Suppress some output\n",
    "}\n",
    "\n",
    "# Train the model:\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=1000,\n",
    "                valid_sets=lgb_eval,\n",
    "                callbacks=[early_stopping(20)])\n",
    "\n",
    "# Make predictions:\n",
    "y_score = gbm.predict(input_test, num_iteration=gbm.best_iteration)\n",
    "y_pred  = [1 if pred > 0.1 else 0 for pred in y_score] #Classify b-quark yes or no. \n",
    "\n",
    "# Print the time usage:\n",
    "end = time.time()\n",
    "print(f\"Time used by LightGBM: {(end-start)*1000:.1f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate:\n",
    "fpr, tpr, _ = roc_curve(truth_test, y_score)                  # False/True Positive Rate for our model\n",
    "fpr_nnbjet, tpr_nnbjet, _ = roc_curve(truth_test, benchmark_test)  # False/True Positive Rate for Aleph NNbjet\n",
    "\n",
    "# We can now calculate the AUC scores of these ROC-curves:\n",
    "auc_score = auc(fpr,tpr)                        # This is the AUC score for our model\n",
    "auc_score_nnbjet = auc(fpr_nnbjet, tpr_nnbjet)  # This is the AUC score for Aleph NNbjet\n",
    "\n",
    "# Let's plot the ROC curves for these results:\n",
    "fig = plt.figure(figsize = [10,10])\n",
    "plt.title('Model ROC Comparison', size = 16)\n",
    "plt.plot(fpr, tpr, label=f'Our LightGBM model (AUC = {auc_score:5.3f})')\n",
    "plt.plot(fpr_nnbjet, tpr_nnbjet, label = f'Aleph NNbjet (AUC = {auc_score_nnbjet:5.3f})')\n",
    "plt.legend(fontsize=16)\n",
    "plt.xlabel('False Postive Rate', size=16)\n",
    "plt.ylabel('True Positive Rate', size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Classify B-jets using MLPclassifier:\n",
    "\n",
    "This is a solution example using MLPclassifier (NN based)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "start=time.time()\n",
    "clf = MLPClassifier(max_iter=20000,\n",
    "                    n_iter_no_change=100,           # Number of iterations without improvement before stopping\n",
    "                    solver='adam',                  # Standard for minimising\n",
    "                    activation='logistic',          # Standard function (but slightly slower)\n",
    "                    hidden_layer_sizes=(10, 10),    # Size of network\n",
    "                    learning_rate='invscaling',     # Other options are \"constant\" and \"adaptive\"\n",
    "                    random_state=42)\n",
    "clf.fit(input_test, truth_test)\n",
    "\n",
    "# Make predictions (NOTE: This \"spits out\" two columns!):\n",
    "y_score_MLP = clf.predict_proba(input_test)\n",
    "\n",
    "# Print the time usage:\n",
    "end = time.time()\n",
    "print(f\"Time used by MLPClassifier: {(end-start)*1000:.1f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluate:\n",
    "fpr, tpr, _ = roc_curve(truth_test, y_score_MLP[:,1])              # False/True Positive Rate for our model\n",
    "fpr_nnbjet, tpr_nnbjet, _ = roc_curve(truth_test, benchmark_test)  # False/True Positive Rate for Aleph NNbjet\n",
    "\n",
    "# We can now calculate the AUC scores of these ROC-curves:\n",
    "auc_score = auc(fpr,tpr)                        # This is the AUC score for our model\n",
    "auc_score_nnbjet = auc(fpr_nnbjet, tpr_nnbjet)  # This is the AUC score for Aleph NNbjet\n",
    "\n",
    "# Let's plot the ROC curves for these results:\n",
    "fig = plt.figure(figsize = [10,10])\n",
    "plt.title('Model ROC Comparison', size = 16)\n",
    "plt.plot(fpr, tpr, label=f'Our MLP model (AUC = {auc_score:5.3f})')\n",
    "plt.plot(fpr_nnbjet, tpr_nnbjet, label = f'Aleph NNbjet (AUC = {auc_score_nnbjet:5.3f})')\n",
    "plt.legend(fontsize=16)\n",
    "plt.xlabel('False Postive Rate', size=16)\n",
    "plt.ylabel('True Positive Rate', size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Classify B-jets using TensorFlow:\n",
    "\n",
    "This is a solution example using TensorFlow (NN based).\n",
    "\n",
    "The example is built with inspiration from\n",
    "https://blog.cmgresearch.com/2020/09/06/tensorflow-binary-classification.html\n",
    "\n",
    "The link contains additional explanitory text and short 5-minute youtube video explaining core concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "#Always split your dataset into train and validation part. We choose a 75:25 division here. Shuffels dataset before split. By using a number (42), it creates a random seed so you can rerun and obtain the same result.\n",
    "input_train, input_valid, truth_train, truth_valid = train_test_split(input_data, truth_data, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create a NN. Loss function is BinaryCrossEntropy. Output layer has 1 node;the prediction for isb. Learning rate defaults to 0.001. \n",
    "model = Sequential([\n",
    "    Dense(9,activation='relu',name='input_layer'),\n",
    "    Dense(24,activation='relu',name='hidden_layer1'),\n",
    "    Dense(12,activation='relu',name='hidden_layer2'),\n",
    "    Dense(1, activation='sigmoid', name='output')])\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.losses.BinaryCrossentropy()])\n",
    "\n",
    "print('--------- TRAINING ---------')\n",
    "history = model.fit(x = np.array(input_train), y = np.array(truth_train), validation_data=(np.array(input_valid), np.array(truth_valid)), epochs = 7)  \n",
    "## This trains the model on input_train by comparing to the true values in truth_train. After every epoch of training, the model is evaluated on the validation dataset, \n",
    "## namely input_valid and truth_valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "\n",
    "fig =  plt.figure()\n",
    "plt.plot(training_loss,label = 'training loss')\n",
    "plt.plot(training_loss,'o')\n",
    "plt.plot(validation_loss, label = 'validation loss')\n",
    "plt.plot(validation_loss, 'o')\n",
    "plt.legend()\n",
    "plt.xticks(size = 12)\n",
    "plt.yticks(size = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on training curve:\n",
    "As you can see, after the 3rd epoch the validation loss and training loss cross each other. This is important! Do you know why? Now we have a trained model and we're ready to make predictions. Usually, one would have a test set (so in total one would have; a training set, a validation set AND a test set). But for simplicity, let's just predict on the validation sample. **This is OK because the model has not trained on this set - if we asked the model to predict on examples on which it has trained, we would be cheating!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now make predictions on input_valid. Notice we're not giving it any truth values!\n",
    "predictions = model.predict(input_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(truth_valid, predictions)      # False Positive Rate and True Positive Rate for our model\n",
    "fpr_nnbjet, tpr_nnbjet, _ = roc_curve(truth_data, benchmark_data) # False Positive Rate and True Positive Rate for ALEPH model\n",
    "\n",
    "# We can now calculate the AUC scores of these ROC-curves:\n",
    "auc_score = auc(fpr,tpr)                         # this is auc score for our model\n",
    "auc_score_nnbjet = auc(fpr_nnbjet, tpr_nnbjet)   # this is the auc score for nnbjet\n",
    "\n",
    "# Plot the results:\n",
    "fig = plt.figure(figsize = [10,10])\n",
    "plt.title('ROC Comparison', size = 18)\n",
    "plt.xlabel('False Postive Rate', size = 18)\n",
    "plt.ylabel('True Positive Rate', size = 18)\n",
    "plt.plot(fpr,        tpr,        label = f'Our model (AUC = {auc_score:6.4f})')\n",
    "plt.plot(fpr_nnbjet, tpr_nnbjet, label = f'nnbjet (AUC = {auc_score_nnbjet:6.4f}')\n",
    "plt.legend(loc='lower right', fontsize=18)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify B-jets using PyTorch:\n",
    "\n",
    "This is a solution example using PyTorch (NN based).\n",
    "\n",
    "The example is built with inspiration from\n",
    "https://towardsdatascience.com/pytorch-tabular-binary-classification-a0368da5bb89\n",
    "\n",
    "The link contains additional explanitory text and short 5-minute youtube video explaining core concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch \"specials\":\n",
    "PyTorch requires that we put this data into the pytorch-Dataset class, such that we can extract it during training.\n",
    "\n",
    "PyTorch generally wants things to be written into functions and classes, which makes it slightly less easy to \"just use\", but once you get acquainted with this, PyTorch is a powerful and versatile tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data:\n",
    "class MyDataset(Dataset):    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.input = X_data\n",
    "        self.truth = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.input[index], self.truth[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.input)\n",
    "#In pytorch, there is an additional step of turning your data into tensors\n",
    "train_data = MyDataset(torch.FloatTensor(np.array(input_train)), \n",
    "                       torch.FloatTensor(np.array(truth_train)))\n",
    "valid_data = MyDataset(torch.FloatTensor(np.array(input_valid)), \n",
    "                       torch.FloatTensor(np.array(truth_valid)))\n",
    "\n",
    "# We can now access input_train via train_data.input and truth_train via train_data.truth,\n",
    "# and similarly for input_valid and truth_valid:\n",
    "print(train_data.input[:2]) #Always check what your data looks like\n",
    "print(train_data.truth[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model:\n",
    "class OurModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OurModel, self).__init__()        # Here we define the layers.\n",
    "        self.input_layer = nn.Linear(6, 24)     #In pytorch, you define the input and output edges explicitly.\n",
    "        self.hidden_layer1 = nn.Linear(24, 24)\n",
    "        self.hidden_layer2 = nn.Linear(24, 12)\n",
    "        self.output_layer = nn.Linear(12, 2) \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, inputs):                  # Here we define how data passes through the layers. \n",
    "        x = self.input_layer(inputs)            # Also here, pytorch is a bit more explicit in defining the layers and activation function separately\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden_layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden_layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop:\n",
    "def Train(model, optimizer, loss_function, train_loader, validation_loader, device, epochs):\n",
    "    validation_loss = []\n",
    "    training_loss   = []\n",
    "    model.train()\n",
    "    for e in range(0, epochs):\n",
    "        epoch_loss = 0\n",
    "        n_minibatches = 0\n",
    "        for input_train_batch, truth_train_batch in train_loader:\n",
    "            input_train_batch, truth_train_batch = input_train_batch.to(device), truth_train_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(input_train_batch)  # This asks our model to produce predictions on the training batch            \n",
    "            loss = loss_function(prediction, truth_train_batch.long())  # This calculates the loss\n",
    "            loss.backward()                                             # This initiates the backpropagation\n",
    "            optimizer.step()                                            # Updates the paramaters\n",
    "            epoch_loss += loss.item()                                   # Extracts the loss value, such that mean loss can be calculated later (see 6 lines down)\n",
    "            n_minibatches += 1\n",
    "        \n",
    "        # Now that the model have trained 1 epoch, we evaluate the model on the validation set:\n",
    "        valid_loss = Validate(model, validation_loader, device, loss_function)\n",
    "        validation_loss.append(valid_loss)\n",
    "        training_loss.append(epoch_loss/n_minibatches)\n",
    "        print('EPOCH: %s | training loss: %s  | validation loss: %s'%(e+1,round(epoch_loss/n_minibatches,3), round(valid_loss, 3)))\n",
    "    return training_loss, validation_loss\n",
    "\n",
    "\n",
    "def Validate(model, validation_loader, device, loss_function):\n",
    "    model.eval()\n",
    "    n_batches  = 0\n",
    "    validation_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for input_valid_batch, truth_valid_batch in validation_loader:\n",
    "            input_valid_batch, truth_valid_batch = input_valid_batch.to(device), truth_valid_batch.to(device)\n",
    "            prediction = model(input_valid_batch)\n",
    "            loss = loss_function(prediction, truth_valid_batch.long())\n",
    "            validation_loss += loss.item()\n",
    "            n_batches += 1\n",
    "    validation_loss = validation_loss/n_batches\n",
    "    return validation_loss\n",
    "\n",
    "\n",
    "def Predict(model, prediction_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    print('PREDICTING!')\n",
    "    with torch.no_grad():\n",
    "        for input_pred_batch, _ in validation_loader:\n",
    "            input_pred_batch = input_pred_batch.to(device)\n",
    "            prediction = model(input_pred_batch)\n",
    "            predictions.extend(prediction.numpy())\n",
    "    print('Done Predicting!')\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now everything is ready, and we thus define the optimisation (hyper-) parameters:\n",
    "learning_rate = 1e-3      # The step size in the direction of \"good\" from stocastic gradient descent (important!)\n",
    "batch_size    = 32        # The size of the batches used for each of the stocastic gradient descent calculations\n",
    "n_epochs      = 8         # Number of epochs, i.e. times that we run through the entire dataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = OurModel() \n",
    "model.to(device)       # Mount the model to the selected device. Either CPU or GPU.\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_function = CrossEntropyLoss()\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(dataset=valid_data, batch_size=batch_size)\n",
    "\n",
    "training_loss, validation_loss = Train(model, optimizer, loss_function, train_loader, validation_loader, device, n_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This trains the model on input_train by comparing to the true values in truth_train.\n",
    "After every epoch of training, the model is evaluated on the validation dataset, namely input_valid and truth_valid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig =  plt.figure()\n",
    "plt.plot(training_loss,label = 'training loss')\n",
    "plt.plot(training_loss,'o')\n",
    "plt.plot(validation_loss, label = 'validation loss')\n",
    "plt.plot(validation_loss, 'o')\n",
    "plt.legend()\n",
    "plt.xticks(size = 12)\n",
    "plt.yticks(size = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = Predict(model, validation_loader, device) # Make predictions on input_valid. Notice we're not giving it any truth values!\n",
    "print(\"\\nRaw predictions: \\n\", predictions[:2])\n",
    "\n",
    "# The output of our model is raw \"logits\" from the final output layer.\n",
    "# This means it produces a pseudo score for each class (a score for 0 and a score for 1). \n",
    "# The function \"expit\" converts this logit to a number in [0,1].\n",
    "# We then combine the logit scores such that our_score = (1_score) / (1_score + 0_score).\n",
    "predictions = pd.DataFrame(predictions)\n",
    "predictions.columns = ['not_bquark', 'bquark']\n",
    "predictions['not_bquark'] = expit(predictions['not_bquark'])\n",
    "predictions['bquark'] = expit(predictions['bquark'])\n",
    "predictions = predictions['bquark']/(predictions['bquark'] + predictions['not_bquark'])\n",
    "\n",
    "print(\"\\nTransformed predictions: \\n\", predictions[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(truth_valid, predictions)      # False Positive Rate and True Positive Rate for our model\n",
    "fpr_nnbjet, tpr_nnbjet, _ = roc_curve(truth_data, benchmark_data) # False Positive Rate and True Positive Rate for ALEPH model\n",
    "\n",
    "# We can now calculate the AUC scores of these ROC-curves:\n",
    "auc_score = auc(fpr,tpr)                         # this is auc score for our model\n",
    "auc_score_nnbjet = auc(fpr_nnbjet, tpr_nnbjet)   # this is the auc score for nnbjet\n",
    "\n",
    "# Plot the results:\n",
    "fig = plt.figure(figsize = [10,10])\n",
    "plt.title('ROC Comparison', size = 18)\n",
    "plt.xlabel('False Postive Rate', size = 18)\n",
    "plt.ylabel('True Positive Rate', size = 18)\n",
    "plt.plot(fpr,        tpr,        label = f'Our model (AUC = {auc_score:6.4f})')\n",
    "plt.plot(fpr_nnbjet, tpr_nnbjet, label = f'nnbjet (AUC = {auc_score_nnbjet:6.4f}')\n",
    "plt.legend(loc='lower right', fontsize=18)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
